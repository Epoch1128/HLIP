{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HLIP Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gcf22/miniconda3/envs/gcf/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "import models\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from utils import make_coord\n",
    "from glob import glob\n",
    "\n",
    "def freq_filter(crop_lr, centers):\n",
    "    frq_list = []\n",
    "    chs, rows, cols = crop_lr.shape\n",
    "    crow, ccol = int(rows/2), int(cols/2)\n",
    "    for center in centers:\n",
    "        mask = np.ones((rows, cols), np.uint8)\n",
    "        mask[crow-center:crow+center, ccol-center:ccol+center] = 0\n",
    "        for ch in range(chs):\n",
    "            f_lr = np.fft.fft2(crop_lr[ch])\n",
    "            f_lr_shift = np.fft.fftshift(f_lr)\n",
    "            f_shift = f_lr_shift * mask\n",
    "            ifft_image = np.fft.ifft2(np.fft.ifftshift(f_shift)).real\n",
    "            frq_list.append((ifft_image - np.min(ifft_image)) / (np.max(ifft_image) - np.min(ifft_image)))\n",
    "    frq_lr = np.stack(frq_list, axis=0).astype(np.float32)\n",
    "    return frq_lr\n",
    "\n",
    "def batched_predict(model, inp, coord, cell, bsize, frq_lr):\n",
    "    with torch.no_grad():\n",
    "        features = model.gen_feat(inp, frq_lr) # B F height width\n",
    "        n = coord.shape[1]\n",
    "        ql = 0\n",
    "        preds = []\n",
    "        while ql < n:\n",
    "            qr = min(ql + bsize, n)\n",
    "            pred = model.query_rgb(features, coord[:, ql: qr, :], cell[:, ql: qr, :])\n",
    "            preds.append(pred)\n",
    "            ql = qr\n",
    "        pred = torch.cat(preds, dim=1)\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = './input'  # Directory to input images\n",
    "model = './save/x4.pth' # Path to inference model\n",
    "# Remind: Our model is input-size and magnification free, so you can select any model for any magnification super resolution. Obviously, the matched model has the best performance.\n",
    "resolution = [256, 256] # Target super resolution size\n",
    "output_dir = './output' # Directory to save super resolution images\n",
    "gpu = 'cuda:0'\n",
    "ext = 'jpg' # Image type\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "model = models.make(torch.load(model)['model'], load_sd=True).to(torch.device(gpu))\n",
    "h, w = resolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]/home/gcf22/miniconda3/envs/gcf/lib/python3.7/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /opt/conda/conda-bld/pytorch_1670525553989/work/aten/src/ATen/native/TensorShape.cpp:3190.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "100%|██████████| 5/5 [00:04<00:00,  1.23it/s]\n"
     ]
    }
   ],
   "source": [
    "for input_image in tqdm(glob(os.path.join(input_dir, f'*.{ext}'))):\n",
    "    lr_img = Image.open(input_image).convert('RGB')\n",
    "    img_name = input_image.split('/')[-1].split('.')[0]\n",
    "    img = transforms.ToTensor()(lr_img)\n",
    "    frq_lr = freq_filter(img, [30, 60])\n",
    "    frq_lr = torch.tensor(frq_lr, dtype=torch.float32)\n",
    "    coord = make_coord((h, w)).cuda()\n",
    "    cell = torch.ones_like(coord)\n",
    "    cell[:, 0] *= 2 / h\n",
    "    cell[:, 1] *= 2 / w\n",
    "    pred = batched_predict(model, ((img - 0.5) / 0.5).cuda().unsqueeze(0),\n",
    "        coord.unsqueeze(0), cell.unsqueeze(0), bsize=30000, frq_lr=frq_lr.cuda().unsqueeze(0))[0]\n",
    "    pred = (pred * 0.5 + 0.5).clamp(0, 1).view(h, w, 3).permute(2, 0, 1).cpu()\n",
    "    transforms.ToPILImage()(pred).save(os.path.join(output_dir, f'{img_name}.jpg'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
